{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Similiarity_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmuQZEVokSO7",
        "colab_type": "code",
        "outputId": "c342d979-9aff-4ae6-edf4-c4839404805d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwRMb8gCjJNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import imageio\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras \n",
        "import cv2\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ilO22HgjoFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(2) # Python\n",
        "np.random.seed(1) #numpy\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(3) # Tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-iVE9CSjsQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#The path to the omniglot data\n",
        "root_path = \"./drive/My Drive/\"\n",
        "train_path = os.path.join(root_path,'Traning') \n",
        "validation_path = os.path.join(root_path,'Evalution')\n",
        "\n",
        "def load_image(path, n = 0):\n",
        "    X = []\n",
        "    \n",
        "    #Load every alphabet seperately and place that in one tensor\n",
        "    for alphabet in os.listdir(path):\n",
        "#         print(\"Loading Alphabet: \" + alphabet)\n",
        "        alphabet_path = os.path.join(path, alphabet)\n",
        "       \n",
        "        category_images = []\n",
        "            \n",
        "        if not os.path.isdir(alphabet_path):\n",
        "                continue\n",
        "            \n",
        "            #Read evey image with in the directory\n",
        "        for filename in os.listdir(alphabet_path):\n",
        "                image_path = os.path.join(alphabet_path, filename)\n",
        "                image = imageio.imread(image_path)\n",
        "                width = 105\n",
        "                height = 105 # keep original height\n",
        "                dim = (height,width)\n",
        " \n",
        "                # resize image\n",
        "                image = cv2.resize(image, dim)\n",
        "                image=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "                image= np.expand_dims(image, axis=0)\n",
        "#                 print(image_path)\n",
        "                #Image preprocessing\n",
        "                image = image/255\n",
        "                image = 1 - image\n",
        "                \n",
        "                X.append(image)\n",
        "                \n",
        "        \n",
        "    X = np.stack(X)\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1XPY1NDwZQ4",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiVe4riutJia",
        "colab_type": "code",
        "outputId": "61830cbd-a77b-4487-d889-eb43373b683a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Loading Training Set\")\n",
        "Xtrain = load_image(train_path)\n",
        "print(Xtrain.shape)"
      ],
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Training Set\n",
            "(1064, 1, 105, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDx2NkDEtbtp",
        "colab_type": "code",
        "outputId": "0158afc5-1bac-486c-e66d-6943e9d2e142",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Now loading evaluation set\")\n",
        "Xval = load_image(validation_path)\n",
        "print(Xval.shape)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now loading evaluation set\n",
            "(1064, 1, 105, 105)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bA7bnLjpkMX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batch(data, batch_size):\n",
        "    n_classes, n_examples, h, w = data.shape\n",
        "    \n",
        "    pairs = [np.zeros((batch_size, 1, h, w)) for i in range(2)]\n",
        "    \n",
        "    targets = np.zeros((batch_size,))\n",
        "    targets[batch_size//2:] = 1\n",
        "    \n",
        "    categories = np.random.choice(n_classes, size = (batch_size), replace = False)\n",
        "    \n",
        "    for i in range(batch_size):\n",
        "        category = categories[i]\n",
        "        \n",
        "        idx1 = np.random.randint(0, n_examples)\n",
        "        pairs[0][i,:,:,:] = data[category, idx1].reshape(1, h,w)\n",
        "        idx2 = np.random.randint(0, n_examples)\n",
        "        \n",
        "        if targets[i] == 0:\n",
        "            category_2 = category\n",
        "        else:\n",
        "            category_2 = (category + np.random.randint(1, n_classes)) % n_classes\n",
        " \n",
        "        \n",
        "        pairs[1][i,:,:,:] = data[category_2, idx2].reshape(1, h, w)\n",
        "        \n",
        "    return pairs, targets\n",
        "        \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gZYIb6Opj6vu",
        "colab": {}
      },
      "source": [
        "def generate(data,batch_size):\n",
        "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
        "        while True:\n",
        "            pairs, targets = get_batch(Xtrain,batch_size)\n",
        "            yield (pairs, targets)    \n",
        "\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQo6r5PdW9bG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_oneshot_task(N,data,language=None):\n",
        "        \"\"\"Create pairs of test image, support set for testing N way one-shot learning. \"\"\"\n",
        "        n_classes, n_examples, w, h = data.shape\n",
        "        indices = np.random.randint(0,n_examples,size=(N,))\n",
        "        categories = np.random.choice(range(n_classes),size=(N,),replace=False)            \n",
        "        true_category = categories[0]\n",
        "        ex1 = np.random.randint(0, n_examples)\n",
        "        ex2 = np.random.randint(0, n_examples)\n",
        "        # ex1, ex2 = np.random.choice(n_examples,replace=False,size=(0,))\n",
        "        test_image = np.asarray([data[true_category,ex1,:,:]]*N).reshape(N, 1, w,h)\n",
        "        support_set = data[categories,indices,:,:]\n",
        "        support_set[0,:,:] = data[true_category,ex2]\n",
        "        support_set = support_set.reshape(N, 1, w,h)\n",
        "        targets = np.zeros((N,))\n",
        "        targets[0] = 1\n",
        "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
        "        pairs = [test_image,support_set]\n",
        "\n",
        "        return pairs, targets\n",
        "    \n",
        "\n",
        "                            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BveJBYSGXBSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_oneshot(model,N,k,data,verbose=0):\n",
        "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
        "        n_correct = 0\n",
        "        if verbose:\n",
        "            print(\"Evaluating model on {} random {} way one-shot learning tasks ...\".format(k,N))\n",
        "        for i in range(k):\n",
        "            inputs, targets = make_oneshot_task(N,data)\n",
        "            probs = model.predict(inputs)\n",
        "            if np.argmax(probs) == np.argmax(targets):\n",
        "                n_correct+=1\n",
        "        percent_correct = (100.0*n_correct / k)\n",
        "        if verbose:\n",
        "            print(\"Got an average of {}% {} way one-shot learning accuracy\".format(percent_correct,N))\n",
        "        return percent_correct\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G7OVaeFXD8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, epochs, verbosity):\n",
        "        model.fit_generator(self.generate(batch_size),)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHye1P61Kwgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense, Input, Lambda\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input\n",
        "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB_F4h-QK4k0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Contrastive Loss\n",
        "def euclid_dist(input_pair):\n",
        "    x, y = input_pair\n",
        "    distance = K.sqrt(K.maximum(K.sum(K.square(x - y), axis=-1, keepdims=True), K.epsilon()))\n",
        "    return distance\n",
        "\n",
        "def euclid_dist_shape(shapes):\n",
        "    shape1, shape2 = shapes\n",
        "    return (shape1[0],1)\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    margin = 1\n",
        "    y_true = -1 * y_true + 1\n",
        "    return K.mean((1-y_true) * K.square(y_pred) + y_true *  K.square(K.maximum(margin - y_pred, 0.0)))\n",
        "\n",
        "def acc(y_true, y_pred):\n",
        "    ones = K.ones_like(y_pred)\n",
        "    return K.mean(K.equal(y_true, ones - K.clip(K.round(y_pred), 0, 1)), axis=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUkf3OQeLdLe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "im_width  =105\n",
        "im_height =105\n",
        "im_chan   =1      # Number of channels: first is original and second cumsum(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dc2DWlYoNDVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f47930f6-22d6-4d93-d598-71f3a837c158"
      },
      "source": [
        "inp =Input(( im_chan,im_height,im_width ), name='img')\n",
        "\n",
        "# print(input_img)\n",
        "# Down Block 1\n",
        "c1 = Conv2D(32,(1,1), activation='relu', kernel_regularizer=l2(2e-4) )(input_img)\n",
        "p1 = MaxPooling2D(data_format=\"channels_first\") (c1)\n",
        "\n",
        "#Down Block 2\n",
        "c2 = Conv2D(128, (1, 1), activation='relu', kernel_regularizer=l2(2e-4)) (p1)\n",
        "p2 = MaxPooling2D(data_format=\"channels_first\") (c2)\n",
        "\n",
        "#Down Block 3\n",
        "c3 = Conv2D(128, (1, 1), activation='relu', kernel_regularizer=l2(2e-4)) (p2)\n",
        "p3 = MaxPooling2D(data_format=\"channels_first\") (c3)\n",
        "\n",
        "#Down Block 4\n",
        "c4 = Conv2D(256, (1, 1), activation='relu', kernel_regularizer=l2(2e-4)) (p3)\n",
        "p4 = MaxPooling2D(data_format=\"channels_first\") (c4)\n",
        "# # print(p4)\n",
        "flat = Flatten()(p4)\n",
        "output = Dense(100, activation='sigmoid')(flat)\n",
        "\n",
        "# Instantiate the Model\n",
        "model = Model(input_img, output)\n",
        "\n",
        "input1 = Input(shape = (1,105,105))\n",
        "input2 = Input(shape = (1,105,105))\n",
        "\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "\n",
        "distance = Lambda(euclid_dist, output_shape=euclid_dist_shape)([output1, output2])\n",
        "\n",
        "prediction = Dense(1, activation='sigmoid')(distance)\n",
        "    \n",
        "siamese = Model(inputs=[input1, input2], outputs=prediction)\n",
        "    \n",
        "optimizer = 'adam'\n",
        "    \n",
        "siamese.compile(loss=contrastive_loss, optimizer=optimizer, metrics=[acc])\n",
        "\n",
        "siamese.summary()"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_41\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_31 (InputLayer)           (None, 1, 105, 105)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_32 (InputLayer)           (None, 1, 105, 105)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model_40 (Model)                (None, 100)          107428      input_31[0][0]                   \n",
            "                                                                 input_32[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 1)            0           model_40[1][0]                   \n",
            "                                                                 model_40[2][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_19 (Dense)                (None, 1)            2           lambda_9[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 107,430\n",
            "Trainable params: 107,430\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCODHWVjRJs4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ab2619e2-4915-4d57-92bb-7303cc3afa0d"
      },
      "source": [
        "#Training loop\n",
        "print(\"!\")\n",
        "evaluate_every = 1 # interval for evaluating on one-shot tasks\n",
        "loss_every=50 # interval for printing loss (iterations)\n",
        "batch_size = 32\n",
        "n_iter = 30\n",
        "N_way = 50 # how many classes for testing one-shot tasks>\n",
        "n_val = 30 #how mahy one-shot tasks to validate on?\n",
        "best = -1\n",
        "weights_path = \"weights\"\n",
        "print(\"training\")\n",
        "for i in range(1, n_iter):\n",
        "    (inputs,targets)=get_batch(Xtrain,batch_size)\n",
        "    loss=siamese.train_on_batch(inputs,targets)\n",
        "    print(loss)\n",
        "    if i % evaluate_every == 0:\n",
        "        print(\"evaluating\")\n",
        "        val_acc = test_oneshot(siamese,N_way,n_val,Xval,verbose=True)\n",
        "        if val_acc >= best:\n",
        "            print(\"saving\")\n",
        "            siamese.save(weights_path)\n",
        "            best=val_acc\n",
        "\n",
        "    if i % loss_every == 0:\n",
        "        print(\"iteration {}, training loss: {:.2f},\".format(i,loss))"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\n",
            "training\n",
            "[0.20824057, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.21306208, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.20071301, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.19737765, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.20325865, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n",
            "[0.18946348, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.19587812, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.19571741, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.18738095, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.19905938, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 93.33333333333333% 50 way one-shot learning accuracy\n",
            "[0.19441572, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 93.33333333333333% 50 way one-shot learning accuracy\n",
            "[0.18750119, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.19125101, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.19164194, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.18770856, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.19183856, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.1909724, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.19004565, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.181007, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.19677168, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.19271152, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.19433957, 0.96875]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.18147695, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.18023989, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.19579688, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 96.66666666666667% 50 way one-shot learning accuracy\n",
            "[0.18496114, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.185237, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.18160759, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 100.0% 50 way one-shot learning accuracy\n",
            "saving\n",
            "[0.18688291, 1.0]\n",
            "evaluating\n",
            "Evaluating model on 30 random 50 way one-shot learning tasks ...\n",
            "Got an average of 90.0% 50 way one-shot learning accuracy\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
